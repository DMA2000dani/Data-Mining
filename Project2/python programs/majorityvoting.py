# -*- coding: utf-8 -*-
"""MajorityVoting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y3iQK9T67acrd8zjAeyJV619_83m8Vtz
"""

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets as ds
import sklearn.model_selection as cv
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import  ConfusionMatrixDisplay,\
                  classification_report,  RocCurveDisplay, PrecisionRecallDisplay,\
                    accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.naive_bayes import BernoulliNB, GaussianNB, CategoricalNB, MultinomialNB
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from IPython.display import display, HTML
show_html = lambda html: display(HTML(html))
from yellowbrick.classifier.rocauc import roc_auc
!pip install skopt
!pip install scikit-optimize
import skopt
from skopt import BayesSearchCV
from sklearn.svm import SVC
from scipy import stats

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("drive/MyDrive/MD/Project_2/Data/Cleaned_Dataset.csv", sep=",", encoding="UTF-8")
df

df = df.dropna()

cls = [str(v) for v in df['Credit_Score'].unique()]
cls

df.columns

X = df.loc[:,df.columns !="Credit_Score"]
y = df["Credit_Score"]

scaler=MinMaxScaler()
X=pd.DataFrame(scaler.fit_transform(X))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

cv=10
iter=40

models = []
alpha = 0.05

"""# Naive Bayes"""

gnb = GaussianNB()

scores = cross_val_score(gnb,X_train,y_train,cv=10)
print("Mean accuracy: {:.2f}%".format(np.mean(scores)*100))
print("Variance: {:.4f}".format(np.var(scores)))

gnb_model = GaussianNB().fit(X_train, y_train)
models.append(gnb_model)
gnb_predictions = gnb_model.predict(X_test)
print(classification_report(gnb_predictions, y_test,target_names=cls))

n = len(gnb_predictions)
mean = np.mean(gnb_predictions)
std_error = stats.sem(gnb_predictions)
confidence_interval = stats.t.interval(1 - alpha, n - 1, loc=mean, scale=std_error)
print("Confidence Interval:", confidence_interval)

plt.figure(figsize=(8,8));
ConfusionMatrixDisplay.from_estimator(gnb_model, X_test,y_test, display_labels=cls, ax=plt.subplot());



"""# KNN"""

knn =  KNeighborsClassifier()
scores = cross_val_score(knn,X_train,y_train,cv=10)
print("Mean accuracy: {:.2f}%".format(np.mean(scores)*100))
print("Variance: {:.4f}".format(np.var(scores)))

param = {'n_neighbors':[1, 3, 5, 7, 11, 15], 
          'weights':['distance', 'uniform'], 
          'leaf_size':[1, 5, 10, 20, 30],
          'metric': ['l2', 'l1', 'cosine']}

knn_gs =  GridSearchCV(knn,param,cv=cv, n_jobs=-1)
knn_gs.fit(X_train, y_train);
models.append(knn_gs)

show_html(pd.DataFrame(knn_gs.cv_results_).loc[:,['params', 'mean_test_score','rank_test_score']].sort_values(by='rank_test_score').head().to_html())

knn_predictions = knn_gs.predict(X_test)
print(classification_report(knn_predictions, y_test,target_names=cls))

n = len(knn_predictions)
mean = np.mean(knn_predictions)
std_error = stats.sem(knn_predictions)
confidence_interval = stats.t.interval(1 - alpha, n - 1, loc=mean, scale=std_error)
print("Confidence Interval:", confidence_interval)

plt.figure(figsize=(8,8));
ConfusionMatrixDisplay.from_estimator(knn_gs, X_test,y_test, display_labels=cls, ax=plt.subplot());

"""# Decision Tree"""

import skopt
from skopt import BayesSearchCV

dt =  DecisionTreeClassifier(random_state=0)
scores = cross_val_score(dt,X_train,y_train,cv=10)
print("Mean accuracy: {:.2f}%".format(np.mean(scores)*100))
print("Variance: {:.4f}".format(np.var(scores)))

param = {'criterion':['gini', 'entropy'], 
         'max_depth':[None, 2, 3, 4, 5, 6, 7, 8, 9],
         'min_samples_leaf':[1,2,3,5,10], 
         'splitter': ['best', 'random'], 
         'max_leaf_nodes':[5, 10, 20, 30]}


dt_bs =  BayesSearchCV(dt,param,n_iter=iter, cv=cv, n_jobs=-1, refit=True, random_state=0)
dt_bs.fit(X_train, y_train);
models.append(dt_bs)

dt_predictions = dt_bs.predict(X_test)
print(classification_report(dt_bs.predict(X_test), y_test,target_names=cls))

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from IPython.display import display, HTML
show_html = lambda html: display(HTML(html))
show_html(pd.DataFrame(dt_bs.cv_results_).loc[:,['params', 'mean_test_score','rank_test_score']].sort_values(by='rank_test_score').head().to_html())

n = len(dt_predictions)
mean = np.mean(dt_predictions)
std_error = stats.sem(dt_predictions)
confidence_interval = stats.t.interval(1 - alpha, n - 1, loc=mean, scale=std_error)
print("Confidence Interval:", confidence_interval)

plt.figure(figsize=(8,8));
ConfusionMatrixDisplay.from_estimator(dt_bs, X_test,y_test, display_labels=cls, ax=plt.subplot());

"""# SVM"""

rbsvc =  SVC(kernel='rbf', max_iter=25000, random_state=0)
scores = cross_val_score(rbsvc,X_train,y_train,cv=10)
print("Mean accuracy: {:.2f}%".format(np.mean(scores)*100))
print("Variance: {:.4f}".format(np.var(scores)))

param = {'C':10**np.linspace(-3,3,101), 'gamma':['scale','auto']}


rbsvc_gs = BayesSearchCV(rbsvc,param,n_iter=iter, cv=cv, n_jobs=-1, refit=True, random_state=0)
rbsvc_gs.fit(X_train, y_train);
models.append(rbsvc_gs)

show_html(pd.DataFrame(rbsvc_gs.cv_results_).loc[:,['params', 'mean_test_score','rank_test_score']].sort_values(by='rank_test_score').head().to_html())

svm_predictions = rbsvc_gs.predict(X_test)
print(classification_report(svm_predictions, y_test,target_names=cls))

n = len(svm_predictions)
mean = np.mean(svm_predictions)
std_error = stats.sem(svm_predictions)
confidence_interval = stats.t.interval(1 - alpha, n - 1, loc=mean, scale=std_error)
print("Confidence Interval:", confidence_interval)

plt.figure(figsize=(8,8));
ConfusionMatrixDisplay.from_estimator(rbsvc_gs, X_test, y_test, display_labels=cls, ax=plt.subplot());

"""# Performance Majority Voting"""

# Combine predictions using majority voting
voting_predictions = []
for i in range(len(X_test)):
    votes = [gnb_predictions[i], knn_predictions[i], dt_predictions[i], svm_predictions[i]]
    majority_vote = max(set(votes), key=votes.count)
    voting_predictions.append(majority_vote)

# Evaluate the majority voting model
accuracy = accuracy_score(y_test, voting_predictions)
print("Mean accuracy: {:.2f}%".format(np.mean(accuracy)*100))
print("Variance: {:.4f}".format(np.var(voting_predictions)))

print(classification_report(voting_predictions, y_test,target_names=cls))

n = len(voting_predictions)
mean = np.mean(voting_predictions)
std_error = stats.sem(voting_predictions)
confidence_interval = stats.t.interval(1 - alpha, n - 1, loc=mean, scale=std_error)
print("Confidence Interval:", confidence_interval)

plt.figure(figsize=(8,8));
ConfusionMatrixDisplay.from_predictions(voting_predictions, y_test)

"""Weighted majority voting"""

# Calculate model accuracies
model_accuracies = [accuracy_score(y_test, model.predict(X_test)) for model in models]

# Normalize accuracies to get weights
weights = [accuracy / sum(model_accuracies) for accuracy in model_accuracies]
print(weights)

weighted_voting_predictions = []
# Combine predictions using weighted voting
for i in range(len(X_test)):
    votes = [gnb_predictions[i], knn_predictions[i], dt_predictions[i], svm_predictions[i]]
    majority_vote = (weights[0] * gnb_predictions[i] +
                      weights[1] * knn_predictions[i] +
                      weights[2] * dt_predictions[i] +
                      weights[3] * svm_predictions[i])
    weighted_voting_predictions.append(round(majority_vote))

print(weighted_voting_predictions)

# Evaluate the weighted voting model
accuracy = accuracy_score(y_test, weighted_voting_predictions)
print("Accuracy:", accuracy)

"""# Random Forest (NOT USED)"""

import skopt
from skopt import BayesSearchCV

rf =  RandomForestClassifier(random_state=0)
scores = cross_val_score(rf,X_train,y_train,cv=10)
print("Mean accuracy: {:.2f}%".format(np.mean(scores)*100))
print("Variance: {:.4f}".format(np.var(scores)))

param = {'n_estimators': [5,10,25,40, 50, 75,100, 200], 
         'criterion':['gini', 'entropy'], 
         'max_depth':[None, 1, 2, 3,  5,  8, 9,10,15],
         'min_samples_leaf':[1,2,3,5,10]}

rf_bs =  BayesSearchCV(rf,param,n_iter=iter, cv=cv, n_jobs=-1, refit=True, random_state=0)
rf_bs.fit(X_train, y_train);

rf_predictions = rf_bs.predict(X_test)
print(classification_report(rf_predictions, y_test,target_names=cls))

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from IPython.display import display, HTML
show_html = lambda html: display(HTML(html))
show_html(pd.DataFrame(rf_bs.cv_results_).loc[:,['params', 'mean_test_score','rank_test_score']].sort_values(by='rank_test_score').head().to_html())

n = len(rf_predictions)
mean = np.mean(rf_predictions)
std_error = stats.sem(rf_predictions)
confidence_interval = stats.t.interval(1 - alpha, n - 1, loc=mean, scale=std_error)
print("Confidence Interval:", confidence_interval)

plt.figure(figsize=(8,8));
ConfusionMatrixDisplay.from_estimator(rf_bs, X_test,y_test, ax=plt.subplot())

# Assuming you have the paired predictions for 8 models
model_predictions = [
    gnb_predictions,
    knn_predictions,
    dt_predictions,
    svm_predictions,
    voting_predictions,
    rf_predictions,
    #model7_predictions,
    #model8_predictions
]

# Create an empty dictionary to store the contingency tables
contingency_tables = {}

# Iterate over the pairs of models
for i in range(len(model_predictions)):
    model1 = model_predictions[i]
    
    for j in range(i+1, len(model_predictions)):
        model2 = model_predictions[j]
        
        # Calculate the values for the contingency table
        a = np.sum(np.logical_and(model1 == 1, model2 == 1))
        b = np.sum(np.logical_and(model1 == 0, model2 == 1))
        c = np.sum(np.logical_and(model1 == 1, model2 == 0))
        d = np.sum(np.logical_and(model1 == 0, model2 == 0))
        
        # Store the contingency table in the dictionary
        table_name = f"Model{i+1}_vs_Model{j+1}"
        contingency_tables[table_name] = [[a, b], [c, d]]

# Print the contingency tables
for table_name, table in contingency_tables.items():
    print(f"Contingency Table for {table_name}:")
    print(np.array(table))
    print()

"""# Confidence Intervals"""

import matplotlib.pyplot as plt

# Confidence intervals of the models
NB_intervals = [2.0416, 2.1852]
KNN_intervals = [2.0514, 2.1903]
DT_intervals = [1.9643, 2.1100]
SVM_intervals = [2.02410, 2.1655]
MV_intervals = [2.0180, 2.1604]
RF_intervals = [2.0144, 2.1566]
AB_intervals = [2.0253, 2.1680]
BG_intervals = [2.0438, 2.1829]

# List of model names for labeling the plot
model_names = ['Naive Bayes', 
               'KNN', 'Dec. Tree', 
               'SVM', 'Maj. Voting',
               'Rand. Forest', 'AdaBoost',
               'Bagging'] 

# Combine the confidence intervals into a list
all_intervals = [NB_intervals, KNN_intervals,
                 DT_intervals, SVM_intervals,
                 MV_intervals, RF_intervals,
                 AB_intervals, BG_intervals]

# Create a box plot of the confidence intervals
plt.figure(figsize=(8, 6))
plt.boxplot(all_intervals, labels=model_names)
plt.xlabel('Models')
plt.ylabel('Confidence Intervals')
plt.title('Comparison of Confidence Intervals')
plt.show()
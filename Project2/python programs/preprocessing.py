# -*- coding: utf-8 -*-
"""Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yzTbAhinRXZ4qibR_RGCjEPt1NUmz0t3

# test.csv

The original data was with the test.csv and train.csv according to the data found in kaggle, but when we load the test.csv, the objective variable doesn't appears, so we can't do any with it. By this reason, we only are going to work with the data in test.csv that is about 100000 instances

# Preprocessing
"""

import pandas as pd
from pandas import plotting
import matplotlib.pyplot as plt
import numpy as np
from sklearn.utils.validation import column_or_1d
from sklearn.feature_selection import VarianceThreshold
import sklearn.utils.random as rd
import seaborn as sns

from sklearn.impute import KNNImputer

"""'ID', 'Customer_ID', 'Name', 'SSN' are all identifiers of some sort and wont help us predict the credit scores of people, hence why they should be dropped.
'Month' shouldn't really have an effect as well even though people might spend differently near christmas or other holidays in general their habits should be visible accross the whole year.
As for 'Occupation', it isn't clear if 'Annual_Income' can give as much information as it so I will keep it for now, being prudent.
"""

from google.colab import drive
drive.mount('/content/drive')

#df = pd.read_csv("drive/MyDrive/MD/Project_2/Data/OriginalDataset.csv", sep=",", encoding="UTF-8",  low_memory = False)
df = pd.read_csv("drive/MyDrive/OriginalDataset.csv", sep=",", encoding="UTF-8",  low_memory = False)

trainFeatureNames=['Age', 'Occupation', 'Annual_Income', "Monthly_Inhand_Salary", "Num_Bank_Accounts", "Num_Credit_Card", "Interest_Rate", "Num_of_Loan", 'Delay_from_due_date', 'Num_of_Delayed_Payment', 'Changed_Credit_Limit', 'Num_Credit_Inquiries', 'Credit_Mix', 'Outstanding_Debt', 'Credit_Utilization_Ratio', 'Credit_History_Age', 'Payment_of_Min_Amount', 'Total_EMI_per_month', 'Amount_invested_monthly', 'Payment_Behaviour', 'Monthly_Balance', 'Credit_Score']
df = df[trainFeatureNames]
df

"""## Treatment of variables that doesn't have information

## Transforming categorical variables to labels
"""

df["Credit_Mix"] = df["Credit_Mix"].replace({"Bad":0, "Standard":1, "Good":2})

df["Payment_of_Min_Amount"] = df["Payment_of_Min_Amount"].replace({"No":0, "Yes":1, "NM":2})

"""## Erasing strange values"""

df = df.replace('_', '', regex=True)
df = df.replace('!@9#%8', '', regex=True)

df

"""# Outliers

**Age outliers**
"""

df['Age'] = df['Age'].astype('float')
sns.boxplot(df['Age'])

df.loc[df['Age'] > 100, 'Age'] = np.nan
df.loc[df['Age'] < 0, 'Age'] = np.nan

sns.boxplot(df['Age'])

"""**Num Bank Accounts**"""

df['Num_Bank_Accounts'] = df['Num_Bank_Accounts'].astype('float')
sns.boxplot(df['Num_Bank_Accounts'])

df.loc[df['Num_Bank_Accounts'] > 15, 'Num_Bank_Accounts'] = np.nan

sns.boxplot(df['Num_Bank_Accounts'])

"""**Num Credit Card**"""

df['Num_Credit_Card'] = df['Num_Credit_Card'].astype('float')
sns.boxplot(df['Num_Credit_Card'])

df.loc[df['Num_Credit_Card'] >= 15, 'Num_Credit_Card'] = np.nan

sns.boxplot(df['Num_Credit_Card'])

"""**Interest Rate**"""

df['Interest_Rate'] = df['Interest_Rate'].astype('float')
sns.boxplot(df['Interest_Rate'])

df.loc[df['Interest_Rate'] > 50, 'Interest_Rate'] = np.nan

sns.boxplot(df['Interest_Rate'])

"""**Num of Loan**"""

df['Num_of_Loan'] = df['Num_of_Loan'].astype('float')
sns.boxplot(df['Num_of_Loan'])

df.loc[df['Num_of_Loan'] < 0, 'Num_of_Loan'] = np.nan
df.loc[df['Num_of_Loan'] > 15, 'Num_of_Loan'] = np.nan

sns.boxplot(df['Num_of_Loan'])

"""**Num of Delayed Payment**"""

df['Num_of_Delayed_Payment'] = df['Num_of_Delayed_Payment'].astype('float')
sns.boxplot(df['Num_of_Delayed_Payment'])

df.loc[df['Num_of_Delayed_Payment'] > 40, 'Num_of_Delayed_Payment'] = np.nan

sns.boxplot(df['Num_of_Delayed_Payment'])

"""**Num Credit Inquiries**"""

df['Num_Credit_Inquiries'] = df['Num_Credit_Inquiries'].astype('float')
sns.boxplot(df['Num_Credit_Inquiries'])

df.loc[df['Num_Credit_Inquiries'] > 25, 'Num_Credit_Inquiries'] = np.nan

sns.boxplot(df['Num_Credit_Inquiries'])

"""**Total EMI per month**"""

df['Total_EMI_per_month'] = df['Total_EMI_per_month'].astype('float')
sns.boxplot(df['Total_EMI_per_month'])

df.loc[df['Total_EMI_per_month'] > 220, 'Total_EMI_per_month'] = np.nan

sns.boxplot(df['Total_EMI_per_month'])

"""**Amount invested monthly**"""

df['Amount_invested_monthly'] = df['Amount_invested_monthly'].astype('float')
sns.boxplot(df['Amount_invested_monthly'])

df.loc[df['Amount_invested_monthly'] > 9000, 'Amount_invested_monthly'] = np.nan

sns.boxplot(df['Amount_invested_monthly'])

"""**Monthly Balance**"""

df['Monthly_Balance'] = df['Monthly_Balance'].astype('float')
sns.boxplot(df['Monthly_Balance'])

df.loc[df['Monthly_Balance'] < -10000, 'Monthly_Balance'] = np.nan

sns.boxplot(df['Monthly_Balance'])

"""# Treatment of the variable with null values
### Looking for variables with null values
"""

print('TRAIN DATASET:')
nullVariables = []
for x in trainFeatureNames:
    nullValues = df[x].isna().sum()
    if(nullValues > 0):
        print("The variable " + x + " have " + str(nullValues) + " null values")
        nullVariables.append(x)

"""## Treatment of the format of Credit_History_Age

Here we will do a transformation of the values in the variable Credit_History_Age from text in years and monts to a value in years
"""

def getN(n):
    string = ''
    num = 0
    for x in n:           
        if(x.isnumeric()):
            string = string + x

    if(string == ''):
        return np.nan
    else:
        return float(string)
    
def getYearAndMonth(instance):
    if(str(instance) == 'nan'):
        return np.nan
    elif(type(instance) == type(1.0)):
        return round(instance, 2)
    else:
        y, m = instance.split('Years')
        year, month = getN(y), getN(m) 
        return round(year + (month/12.), 2)

var = ['Credit_History_Age']
dfCHA = df[var].copy()
dfCHA[var]

varV = []
for x in dfCHA['Credit_History_Age']:
    num = getYearAndMonth(x)
    varV.append(num)

dfCHA['Credit_History_Age'] = varV
df['Credit_History_Age'] = varV

"""## Transformating numerical variables with null Values in the correct type and form"""

df[nullVariables]

def getNum(seq):
    string = ''
    num = 0
    negative = False
    for x in seq:           
        if(x.isnumeric() or x == '.'):
            string = string + x
    if('-' in seq):
            negative = True
    if('.' in seq):
        num, dec = string.split('.')
        num = float(num)
        if(len(dec) > 0):
            num += float(dec[0])/10.0
        if(len(dec) > 1):
            num += float(dec[1])/100.0
        if(len(dec) > 2):
            num += float(dec[2])/1000.0
    else:
        if(string == ''):
            return np.nan
        elif(negative):
            return -float(string)
        else:
            return float(string)
    if(string == ''):
        return np.nan
    elif(negative):
            return -float(string)
    else:
        return round(num,2)

numVars = ["Monthly_Inhand_Salary", 'Num_of_Delayed_Payment', 'Num_Credit_Inquiries', 'Amount_invested_monthly', 'Monthly_Balance']
numericalDF = df[numVars].copy()
numericalDF

for name in numVars:
    varV = []
    for x in range(0, 100000):
        value = numericalDF[name][x]
        if(type(value) == type('string')):
            value = getNum(value)
        varV.append(value)
    #print(varV)
    numericalDF[name] = varV
    df[name] = varV

df[numVars]

"""# Here we will do the NA imputation

# Numerical variables imputation

## Dropping the rows with NA values
"""

realNumVars = ['Monthly_Inhand_Salary', 'Num_of_Delayed_Payment',
       'Num_Credit_Inquiries', 'Amount_invested_monthly',
       'Monthly_Balance', 'Credit_History_Age', 'Age',
       'Num_Bank_Accounts', 'Num_Credit_Card', 'Interest_Rate',
       'Num_of_Loan',
       'Total_EMI_per_month']
realNumVars

dfWithoutNA = df[realNumVars].dropna().copy()
for x in realNumVars:
    print("The size of the varialbe " + x + " is " + str(len(dfWithoutNA[x])))

"""As we can see, dropping the rows of NA values is not a good idea because we loose 54% of our data.
Let's try with other type of imputation

## Mean imputation
"""

numericalVariables = realNumVars
meanVectorVariables = []
for x in numericalVariables:
    suma = df[x].dropna().sum()
    size = len(df[x].dropna())
    mean = suma/size
    meanVectorVariables.append(mean)
    
meanVectorVariables

dfMean = df[realNumVars].copy()
for x in range(0,len(numericalVariables)):
    var = numericalVariables[x]
    dfMean[var] = dfMean[var].replace({np.nan : meanVectorVariables[x]})
for x in realNumVars:
  print(x, dfMean[x].isna().sum())

dfMean

min = dfMean.min()
max = dfMean.max()
minL = []
maxL = [] 
for x in min:
  minL.append(x)
for x in max:
  maxL.append(x)

rangeV = []
binsV = []
size = len(realNumVars)
count = 0
while(count < size):
  rangeV.append((min[count], max[count]))
  binsV.append(30)
  count+=1

binsV[2] = 10
binsV[7] = 10
binsV[8] = 10
binsV[10] = 10

dfMean[realNumVars]

for x in range(0,len(realNumVars)):
    fig = plt.subplots(nrows=1, ncols=1)
    dfMean[realNumVars[x]].hist(bins=binsV[x], range=rangeV[x])
    df[realNumVars[x]].dropna().hist(bins=binsV[x], range=rangeV[x]).set_title(numericalVariables[x])

"""We can see above two histograms. Each one is the comparation with the same variable between the distribution dropping NA valuesin orange colour and mean imputation in blue colour.

As we can seee, the mean imputation doesn't looks a good idea because there are so many NULL values and it can affects to the distribution, specially with thoose who have a higher value of NA. Thoose are the case of the two first histograms. We did the mean imputation using outliyers and its affects on it. For example in the second one, we have a clearly normal distribution, but with the influence of outylers, it will not be as perfect as before. Or for example, in the last one, we can see on the right the majority of the values and on the left the imputation of thoose values that are now outlyers.
"""

df[nullVariables]

"""## KNN imputation"""

## This cell will takes over 12 minutes

imputer = KNNImputer(n_neighbors=5, weights='distance')
KNNdf = imputer.fit_transform(df[numericalVariables])

dfKNN = pd.DataFrame()
for index in range(0, len(realNumVars)):
    varV = []
    for x in KNNdf:
        varV.append(x[index])    
    dfKNN[realNumVars[index]] = varV

dfKNN.describe()

min = dfKNN.min()
max = dfKNN.max()
rangeV = []
binsV = []
size = len(realNumVars)
count = 0
while(count < size):
  rangeV.append((min[count], max[count]))
  binsV.append(30)
  count+=1

binsV[2] = 10
binsV[7] = 10
binsV[8] = 10
binsV[10] = 10

for x in range(0,len(realNumVars)):
    fig = plt.subplots(nrows=1, ncols=1)
    dfKNN[realNumVars[x]].hist(bins=binsV[x], range=rangeV[x]).set_title(realNumVars[x])
    df[realNumVars[x]].dropna().hist(bins=binsV[x], range=rangeV[x])

"""We can see the five histograms comparing the same variable between KNN imputation and the data deleting the rows of each NA values.
As we can see, the distribution is more or less the same in all variables and it is the bests results if we compare the 2 oter methods used before, so we will keep it.
"""

df['Credit_Score'] = df['Credit_Score'].replace({'Good':3})
df['Credit_Score'] = df['Credit_Score'].replace({'Standard':2})
df['Credit_Score'] = df['Credit_Score'].replace({'Poor':1})

"""One Hot Encoding"""

# ONE HOT ENCODING
#one_hot_encoded = pd.get_dummies(df['Occupation'])
#df = df.drop(['Occupation'], axis=1)
#df = pd.concat([df, one_hot_encoded], axis=1)

#one_hot_encoded = pd.get_dummies(df['Payment_Behaviour'])
#df = df.drop(['Payment_Behaviour'], axis=1)
#df = pd.concat([df, one_hot_encoded], axis=1)

df['Occupation'] = df['Occupation'].astype('category').cat.codes
df['Payment_Behaviour'] = df['Payment_Behaviour'].astype('category').cat.codes
df

"""## Solving the higher number of values in the data"""

df = df.dropna()
corr_matrix = df.corr()
sns.heatmap(corr_matrix, annot=True)
plt.show()
print(corr_matrix['Credit_Score'])

df = df.sample(frac=0.15, random_state=42)
X = df.loc[:,df.columns !="Credit_Score"]
y = df["Credit_Score"]

countPoor = 0
countStandard = 0
countGood = 0
for x in y:
  if(x == 1):
    countPoor +=1
  elif(x == 2):
    countStandard += 1
  else:
    countGood += 1

print(countPoor)
print(countStandard)
print(countGood)

from imblearn.under_sampling import RandomUnderSampler

# Supongamos que 'X' es tu matriz de características y 'y' es el vector de etiquetas/clases

# Crear una instancia del RandomUnderSampler
rus = RandomUnderSampler(random_state=42)
X, y = rus.fit_resample(X, y)
print("Distribución de clases después del undersampling:")
print(pd.Series(y).value_counts())

df = pd.concat([X,y],axis=1)

"""## Exporting the data"""

df.to_csv('drive/MyDrive/MD/Project_2/Data/Cleaned_Dataset.csv', index=False)